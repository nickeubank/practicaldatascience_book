{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prediction (Supervised Learning): Classification and Regression\n",
    "\n",
    "## What is prediction and supervised learning?\n",
    "\n",
    "**Prediction** is the process of estimating the value of a variable based on data. Examples of prediction include, estimating the sales price of a home based on its physical characteristics, determining whether an email is spam or not based on the content and metadata (e.g. sender's address) of the message, determining whether or not a photograph contains a cat. These are all examples of prediction. \n",
    "\n",
    "Here, we're using the term \"prediction\" a bit differently than it may be used colloquially, since it's often used as a term to describe a **forecast** - some sort of future prediction. This is a common misconception of predictions, that they estimate something that is meant to occur in the future. A prediction does not have to be of a future state. In fact, none of the examples shared above are the prediction of some future state. \n",
    "\n",
    "Predictions are core to **supervised learning**. Supervised learning is the machine learning subfield of learning from examples to make predictions on unseen data. Let's start with an example to make this clearer.\n",
    "\n",
    "Suppose you wanted to predict the value of a house based on characteristics that describe it. This is a prediction problem. In this case, our output  or **target** variable, $y$ represents the value of the house that we're trying to predict. And we want to predict this with our input, $\\mathbf{x}=[x_1, x_2, x_3]$, which may represent the number of bedrooms ($x_1$), the age of the home ($x_2$), or the presence of a swimming pool ($x_3$). We typically call these inputs **features**. We then use a prediction algorithm ($f$) to estimate the value of our target variable, $y$. Mathematically, we represent this as $y = f(x_1,x_2,x_3)$. For prediction, we need to build the function $f$ that accurately predicts the target variable we care about, $y$. One sample of data where we have the features and target variable for one house, for example, is known as an **observation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training and testing\n",
    "\n",
    "There are at least two steps for any prediction problem: training and testing. Training is the process of giving the prediction algorithm **labeled** examples to learn from that include both the inputs and the outputs. For example, our **training data** (data that includes both the features and the target variable) for the housing price prediction example could be as follows, showing three feature variables ($x_1$,$x_2$,$x_3$) and one target variable ($y$). There are four observations, each representing a separate house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Table 1. Example of training data.*\n",
    "\n",
    "| $x_1$<br>Number of bedrooms| $x_2$<br>Year built | $x_3$<br>Swimming pool present? | $y$<br>Price (\\$) |\n",
    "| ----- | ------ | ---- | ---- |\n",
    "| 2 | 1965 | True  | 325,000 |\n",
    "| 1 | 1957 | False | 297,000 |\n",
    "| 3 | 2004 | False | 443,000 |\n",
    "| 4 | 2023 | True  | 502,000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that for training data, we know both the input values AND the output values. By knowing both we allow the algorithm to learn from those data in the hope of being able to generalize to new, unseen data for which we do not have the house price (output) data and we use the prediction algorithm (that you'll be coding up shortly!), $y$, to learn from it to make accurate predictions.\n",
    "\n",
    "There are two primary categories of prediction: **classification** (for discrete variables) and **regression** (for continuous variables). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Is an email spam or not? Will a stock price go up, down, or stay the same? Is the image a picture of a dog, a cat, or a frog? These are all examples of categorical predictions which can be made through the process of classification. Whenever we are asking yes or no questions or choosing from options in a list, we're performing classification. For classification, the output variable, $y$, is categorical. Each of the options that could be estimated (e.g. spam, not spam) are the **class** labels that can be predicted. This concept is separate from the object oriented class that we have learned about in programming.\n",
    "\n",
    "We typically think of classification when we are making estimates to answer questions like:\n",
    "- Is it _____ or _____?\n",
    "- True or false?\n",
    "- What type of _____ is it?\n",
    "\n",
    "## Regression\n",
    "\n",
    "How much will the home sell for? How tall will the child grow to be? These are examples of regression problems when the output variable, $y$, is continuous.\n",
    "\n",
    "We typically think of regression when we are making estimates to answer questions like:\n",
    "- What is the cost of ______?\n",
    "- How many _____ are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology Review\n",
    "\n",
    "We introduced many new terms here - let's review them below before moving on to the next topic:\n",
    "\n",
    "- **Prediction**. The estimate made by a classification or regression algorithm. For a classifier, this is an estimated class label. For a regression algorithm, this is a numerical estimate. Effective predictions will generalize well to data that were not part of the training dataset. In this data science context, a prediction is not necessarily about an event in the future.\n",
    "- **Supervised Learning**. The process of learning to generalize to make predictions on unseen data from labeled data examples (training data). There are two primary branches of supervised learning: classification (for categorical predictions) and regression (for numerical predictions). Supervised learning is one branch of machine learning.\n",
    "- **Forecasting**. The process of making predictions about events in the future.\n",
    "- **Classification**. The process of making predictions of categorical variables. If there are two categories, it is known as binary classification, if more, it is known as multiclass classification.\n",
    "- **Class**. For classification, this is the outcome variable of interest.\n",
    "- **Regression**. The process of making predictions of numerical variables.\n",
    "- **Feature variable(s)** ($x$ values). The variable or set of variables that you are using to make a prediction. This can either be a single value (e.g. sepal width) or multiple values that collectively represent an observation (e.g. sepal width and petal width). It is also known as the independent variable, input variable, or predictor. \n",
    "- **Target variable** ($y$ values). The quantity you are trying to predict. It is categorical for classification and numerical for regression. It is also known as the outcome or output variable, response variable, dependent variable, or predictand. \n",
    "- **Labeled data**. This means we include BOTH the feature variable data AND corresponding the target variable data for every observation in the dataset. Unlabeled data implies that only the feature variable data are used. Target variable data may not be used or included. \n",
    "- **Training data**. This is a collection of labeled data for a set of observations used to prepare the model to make predictions. \n",
    "- **Test data**. This is a labeled dataset held out from the training process used for evaluating model performance. The prediction algorithm makes predictions of the target variable for the features of the test data and the performance is compared to the target variable for the training data.\n",
    "- **Model training**. The process of fitting our model to our training data such that it is able to generalize to data held out from the training dataset, such as our test data or unlabeled feature variable data.\n",
    "- **Observations**. Also known as samples, these are examples of data. \n",
    "\n",
    "<!-- *Table 1. Example of training data*\n",
    "\n",
    "| Term | Definition | Example |\n",
    "| ----- | ------ | ---- |\n",
    "| Prediction | The estimate made by a classification or regression algorithm. For a classifier, this is an estimated class label. For a regression algorithm, this is a numerical estimate. Effective predictions will generalize well to data that were not part of the training dataset. In this data science context, a prediction is not necessarily about an event in the future. | Estimating whether the measurements of a flower suggest it was part of the iris setosa, versicolor, or virginica variety | -->\n",
    "\n",
    "Now that we have the basic concepts of prediction, let's dive into a tool for prediction: the k nearest neighbors classifier, which you will learn to program yourself!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
