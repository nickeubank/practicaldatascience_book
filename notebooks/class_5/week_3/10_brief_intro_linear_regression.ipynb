{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: A Brief Introduction\n",
    "\n",
    "Suppose you are given the data in the picture below and asked to draw a straight line that you feel best describes the relationship between the square footage of a house (on the x-axis) and its price (on the y-axis). What might that line look like? Seriously, pause for a moment and imagine a line on that graph. What would it look like?\n",
    "\n",
    "[figure with point cloud in approximate line]\n",
    "\n",
    "\n",
    "Most of us would probably end up drawing a line that looks something like the line in the figure below. And that, in a basic sense, is precisely what we are trying to do with linear regression: use mathematics to estimate a \"line of best fit\" that (we hope) gives a pretty good summary of the relationship between different variables.\n",
    "\n",
    "[figure with red line]\n",
    "\n",
    "But how exactly does linear regression do this? Odds are, you probably aren't even quite sure what guided you to think about a line very similar to the red one in the figure above — you just followed your intuition. But the basic idea that probably guided how you drew the line in your head is very similar to the principle used by a linear regression: try to draw a line that, on average, is as close as possible to all the data points plotted.\n",
    "\n",
    "To be more specific, a linear regression estimates the line that minimizes the sum of squared errors between the line of best fit and each data point. Indeed, linear regression is often called \"Ordinary Least Squares\" or \"Least Squares Regression\" precisely because it tries to find the line that minimizes (gives rise to the smallest or least) sum of squared errors. There are reasons that linear regression minimizes the sum of *squared* errors (instead of just the sum of errors), but those reasons aren't crucial to getting an intuitive sense of how linear regression works.\n",
    "\n",
    "## Representing A Regression Line\n",
    "\n",
    "While this kind of picture is the easiest way to visualize a simple regression, this is not how most regressions are presented for reasons we'll discuss below. Instead, regressions generally take advantage of the fact that a line can be represented with an intercept (where the line crosses the y-axis) and a slope (the amount the line rises when you move one unit along the x-axis). In math notation, this generally gets written something like:\n",
    "\n",
    "$$\\text{price} = \\alpha + \\beta * \\text{square footage} + \\epsilon$$\n",
    "\n",
    "The variable we're trying to explain (here, price) is on the left-hand side of the equation, and we write that the price of a house is equal to a constant term (the intercept, the value of $\\alpha$) plus the houses square footage times the slope of the line of best fit ($\\beta$). The last term — $\\epsilon$ — is the error associated with a given observation (the difference between the value of the line of best fit for a given house and the house's true price). Mathematically, it works out that the sum of all the error terms ($\\epsilon$) from a regression will always add up to zero.\n",
    "\n",
    "So suppose we ran a regression, and the regression model estimated that $\\alpha = SOMETHING$ and $\\beta = SOMETHING$. From this, we could conclude that the model's estimate is that a 1,500 square foot house would have a price of ..... From this model, we could also infer that if someone owned a 1,500 square foot house and was thinking of building an extension that would add 500 square feet to the house, then the model's best guess would be that the price of the house would increase by $500 * \\beta = SOMETHING$.\n",
    "\n",
    "While the equation above shows us how regressions are often written out in books or papers, that's not quite how regression models are presented in Python. In Python, this regression would look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression\n",
    "\n",
    "Up until now, we've only looked at regressions in the context of two variables: one we're trying to understand (the \"dependent variable\") and one we think helps to explain variation in the first (our \"explanatory variable\"). But this doesn't really tell the whole story of linear regression. Indeed, what makes linear regressions powerful is not their ability to model the relationship between two variables, but between a single dependent variable and an arbitrary number of explanatory variables that we think *jointly and simulataneously* explain the variation we observe in the dependent variable. And it is for that reason that linear regression is also often called \"multiple regression.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Regression Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Lottery</td>     <th>  R-squared:         </th> <td>   0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   22.20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 04 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>1.90e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:10:29</td>     <th>  Log-Likelihood:    </th> <td> -379.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    86</td>      <th>  AIC:               </th> <td>   765.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    83</td>      <th>  BIC:               </th> <td>   773.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>  246.4341</td> <td>   35.233</td> <td>    6.995</td> <td> 0.000</td> <td>  176.358</td> <td>  316.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Literacy</th>        <td>   -0.4889</td> <td>    0.128</td> <td>   -3.832</td> <td> 0.000</td> <td>   -0.743</td> <td>   -0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(Pop1831)</th> <td>  -31.3114</td> <td>    5.977</td> <td>   -5.239</td> <td> 0.000</td> <td>  -43.199</td> <td>  -19.424</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.713</td> <th>  Durbin-Watson:     </th> <td>   2.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.156</td> <th>  Jarque-Bera (JB):  </th> <td>   3.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.487</td> <th>  Prob(JB):          </th> <td>   0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.003</td> <th>  Cond. No.          </th> <td>    702.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &     Lottery      & \\textbf{  R-squared:         } &     0.348   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.333   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     22.20   \\\\\n",
       "\\textbf{Date:}             & Tue, 04 Jun 2024 & \\textbf{  Prob (F-statistic):} &  1.90e-08   \\\\\n",
       "\\textbf{Time:}             &     12:10:29     & \\textbf{  Log-Likelihood:    } &   -379.82   \\\\\n",
       "\\textbf{No. Observations:} &          86      & \\textbf{  AIC:               } &     765.6   \\\\\n",
       "\\textbf{Df Residuals:}     &          83      & \\textbf{  BIC:               } &     773.0   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}       &     246.4341  &       35.233     &     6.995  &         0.000        &      176.358    &      316.510     \\\\\n",
       "\\textbf{Literacy}        &      -0.4889  &        0.128     &    -3.832  &         0.000        &       -0.743    &       -0.235     \\\\\n",
       "\\textbf{np.log(Pop1831)} &     -31.3114  &        5.977     &    -5.239  &         0.000        &      -43.199    &      -19.424     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  3.713 & \\textbf{  Durbin-Watson:     } &    2.019  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.156 & \\textbf{  Jarque-Bera (JB):  } &    3.394  \\\\\n",
       "\\textbf{Skew:}          & -0.487 & \\textbf{  Prob(JB):          } &    0.183  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.003 & \\textbf{  Cond. No.          } &     702.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                Lottery   R-squared:                       0.348\n",
       "Model:                            OLS   Adj. R-squared:                  0.333\n",
       "Method:                 Least Squares   F-statistic:                     22.20\n",
       "Date:                Tue, 04 Jun 2024   Prob (F-statistic):           1.90e-08\n",
       "Time:                        12:10:29   Log-Likelihood:                -379.82\n",
       "No. Observations:                  86   AIC:                             765.6\n",
       "Df Residuals:                      83   BIC:                             773.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "Intercept         246.4341     35.233      6.995      0.000     176.358     316.510\n",
       "Literacy           -0.4889      0.128     -3.832      0.000      -0.743      -0.235\n",
       "np.log(Pop1831)   -31.3114      5.977     -5.239      0.000     -43.199     -19.424\n",
       "==============================================================================\n",
       "Omnibus:                        3.713   Durbin-Watson:                   2.019\n",
       "Prob(Omnibus):                  0.156   Jarque-Bera (JB):                3.394\n",
       "Skew:                          -0.487   Prob(JB):                        0.183\n",
       "Kurtosis:                       3.003   Cond. No.                         702.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "dat = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "results = smf.ols(\"Lottery ~ Literacy + np.log(Pop1831)\", data=dat).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to Learn More?\n",
    "\n",
    "Great! Our colleague from the statistics department — Mine Çetinkaya-Rundel — has developed an entire course on linear regression and modeling that we think is terrific (and judging by the ratings the course has received, past students do too!). You can check it out here: [Linear Regression and Modeling](https://www.coursera.org/learn/linear-regression-model). The course uses R when they do actual coding, but the focus of the class is on how linear regression works and how results can be interpreted, which is the same whether you're using R, Python, or doing the matrix algebra on a napkin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
