# Linear Regression in Python

The focus of this week's readings and exercises is how to implement linear regressions in Python. Linear regression — also often referred to as multiple regression, least squares regression, Ordinary Least Squares (OLS), and a handful of other things just to make life difficult — is the first data science tool encountered by most students and even our modern world of increasingly sophisticated models, it remains one of the most useful.

As discussed in our first week, up until this course, the *Python Programming for Data Science* specialization in which this course is situated has been focused on programming and has only required that learners be familiar with basic statistical concepts like means, medians, and standard deviations. But we have to be a little more creative in this course because we want to discuss how different statistical methods — like linear regression — are implemented in Python, and we want to serve *both* students who have never seen a linear regression before *and also* learners who know linear regressions backwards and forwards and just want to know how to implement them in Python.

For learners who are new to linear regressions, this week's readings begin with an introduction to the logic and purpose of linear regression — *Linear Regression: A Brief Introduction*. This reading will not be a substitute for a good course on linear regression, but it should provide a sense of how linear regressions work and how they are used. This reading is followed by *Categorical Variables, Indicator Variables, and Linear Regression*, a reading that discusses how to work with a particular type of data in linear regressions.

The goal of these readings is not to make students new to the topic experts but rather to provide enough context to help new students understand the rest of the week's readings on *implementing* regressions in Python. Then when you later take a course that covers linear regressions — and we encourage you do so! — your experience implementing regressions will make that part of the class easy, allowing you to focus more on the theory underlying linear regression.

With that said, there will be a few places in the readings for this week when we discuss how to use more advanced techniques in Python that new readers may not follow. When we do so, we will say that is what we are doing explicitly, and we won't test you on any techniques that we have not explained — we are simply including those to ensure that we provide learners with more experience with data modeling a road map to explore the methods they may wish to implement.

If you have more experience with the theory of linear regression and/or how to implement regressions in a different programming language (e.g., R or Stata), then you may wish to skip this week's first two readings (*Linear Regression: A Brief Introduction* and *Categorical Variables, Indicator Variables, and Linear Regression*). But from that point forward, our focus will be on how different models are implemented in Python, which is hopefully what you came here to learn. Most readings will focus on the central libraries for fitting linear regressions in Python, but because everyone has that one method they just can't live without, we'll also try and provide guidance on where to go to find resources for different use cases.

**If you're a Bayesian:** we won't be covering Bayesian modeling in this course, but worry not — Python has some really terrific Bayesian modeling tools. [PyMC](https://www.pymc.io/welcome.html) is a mature, fully developed, Python-native probabilistic programming language. And if you're more of a Bayesian lightweight and are used to tools like `rstanarm` from the R world, you can jump to [Bambi](https://bambinos.github.io/bambi/), a higher-level modeling library built on top of PyMC.
