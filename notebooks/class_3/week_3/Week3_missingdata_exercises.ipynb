{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Exercises\n",
    "\n",
    "**Note**: \n",
    "\n",
    "> This exercise has been written out in something called a Jupyter Notebook. We'll discuss Jupyter Notebooks in more detail later in this specialization—they are very a powerful tool for data science communication!—but for the time being, the notebook is just a convenient way for us to write out the exercise. You don't need to *do* anything with the notebook except read its contents—just use write your Python code in a regular `.py` file.\n",
    "\n",
    "**WARNING:**\n",
    "\n",
    "> When asked to round your answers to a certain number of decimals, do *not* round any results until you've finished your computations and have your final answer! For example, if you were to calculate the average hourly wage for workers, and you did so by first calculating the average weekly salary of workers and the average hours worked per week, then divided the first number by the second, you should NOT round the average weekly salary of workers or the average hours worked per week. Rounding intermediate results can lead to compounding errors that cause problems for the autograder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Today, we will be using the ACS data we used during out first `pandas` exercise to examine the US income distribution, and how it varies by race. Note that because the US income distribution has a very small number of people with *extremely* high incomes, and the ACS is just a sample of Americans, the far right tail of the distribution will not be very well estimated. However, this data should suffice for helping to understand wealth inequality in the United States. \n",
    "\n",
    "To begin, load the ACS Data in this exercise (the file is called `US_ACS_2017_10pct_sample.dta`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's begin by calculating the mean US incomes from this data (recall that income is stored in the `inctot` variable). \n",
    "\n",
    "Please round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Hmmm... That doesn't look right. The average American is definitely not earning that much a year! Let's look at the values of `inctot` using `value_counts()`. Do you see a problem?\n",
    "\n",
    "Now use `value_counts()` with the argument `normalize=True` to see the proportion of the sample that reports each value instead of the count of people in each category. \n",
    "\n",
    "What share of our sample has an income of 9,999,999? Please round that value (already between 0 and 1) to three decimal places. \n",
    "\n",
    "What share has an income of 0? Please round that value (already between 0 and 1) to three decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "As we discussed before, the ACS uses a value of 9999999 to denote that income information is not available for someone. The problem with using this kind of \"sentinel value\" is that pandas doesn't understand that this is supposed to denote missing data, and so when it averages the variable, it doesn't know to ignore 9999999. \n",
    "\n",
    "To help out `pandas`, use the `replace` command to replace all values of 9999999 with `np.nan`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Now that we've properly labeled our missing data as `np.nan`, let's calculate the average US income once more.\n",
    "\n",
    "Please round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "OK, now we've been able to get a reasonable average income number. As we can see, a major advantage of using `np.nan` is that `pandas` knows that `np.nan` observations should just be ignored when we are calculating means. \n",
    "\n",
    "But it's not enough to just get rid of the people who had `inctot` values of 9999999. We also need to know why those values were missing. Suppose, for example, that the value of 9999999 was used for anyone who made more than 100,000 dollars: if we just dropped those people, then our estimate of average income wouldn't mean much, would it?\n",
    "\n",
    "So let's make sure we understand *why* data is missing for some people. If you recall from our last exercise, it seemed to be the case that most of the people who had incomes of 9999999 were children. Let's make sure that's true by looking at the distribution of the variable `age` for people for whom `inctot` is missing (i.e. subset the data to people with `inctot` missing, then look at the values of `age` with `value_counts()`).\n",
    "\n",
    "Then do the opposite: look at the distribution of the `age` variable for people for whom `inctot` is *not* missing. \n",
    "\n",
    "Can you determine when 9999999 was being used? Is it ok we're excluding those people from our analysis?\n",
    "\n",
    "Note: In this data, Python doesn't understand `age` is a number; it thinks it is a string because the original data has categories like \"90 (90+ in 1980 and 1990)\" and \"less than 1 year old\". So you can't just use `min()` or `max()`. We'll discuss converting string variables into numbers in a future class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Great, so now we know why those people had missing data, and we're ok with excluding them. \n",
    "\n",
    "But as we previously noted, there are also a lot of observations of zero income in our data, and it's not clear that we want everyone with a zero-income *should* be included in this average, since those may be people who are retired, or in school. \n",
    "\n",
    "Let's limit our attention to people who are currently working by subsetting to only employed respondents. We can do this using `empstat`. Remember you can use `value_counts()` to see what values of `empstat` are in the data!\n",
    "\n",
    "How many observations are in your data once you subset for employed respondents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "\n",
    "Now let's estimate the racial income gap in the United States. \n",
    "\n",
    "What is the average salary for employed Black Americans? Please round your answer to two decimal places.\n",
    "\n",
    "What is the average salary for employed White Americans?  Please round your answer to two decimal places.\n",
    " \n",
    "How much more does the average White American make than the average Black American (in percentage terms)? (That is, what is the White average salary minus the Black average salary divided by the Black average salary time 100). You should get a number between 0 and 100. Please round it to one decimal place.\n",
    "\n",
    "**Note:** these values are not quite accurate estimates. As we'll discuss in later lessons, to get completely accurate estimates from the ACS we have to take into account how people were selected to be interviewed. But you get pretty good estimates in most cases even without weights—your estimate of the racial wage gap without weights is within 5\\% of the corrected value. \n",
    "\n",
    "**Note:** This is actually an underestimate of the wage gap. The US Census treats Hispanic respondents as a sub-category of \"White.\" While all ethnic distinctions are socially constructed, and so on some level these distinctions are all deeply problematic, this coding is inconsistent with what most Americans think of when they hear the term \"White,\" a term *most* Americans think of as a category that is mutually exclusive of being Hispanic or Latino (categories which are also usually conflated in American popular discussion). With that in mind, most researchers working with US Census data split \"White\" into \"White, Hispanic\" and \"White, Non-Hispanic\" using `race` *and* `hispan`. But for the moment, just identify \"White\" respondents using the value in `race`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "\n",
    "As noted above, these estimates are not actually *quite* correct because we aren't taking into account the fact that when the US Census decided who to survey, not all Americans had the same likelihood of being asked. The US American Community Survey is an example of a *weighted* survey (essentially, people from smaller subpopulations have a higher likelihood of being included to ensure enough individuals in the final survey to constitute a representative sample that can be used statistically). \n",
    "\n",
    "To calculate a weighted average that takes into account these survey weights (in other words, a more accurate estimate of US incomes), you need to use the following formula:\n",
    "\n",
    "$$weighted\\_mean\\_of\\_x = \\frac{\\sum_i x_i * weight_i}{\\sum_i weight_i}$$\n",
    "\n",
    "(As you can see, when $weight_i$ is constant for all observations, this just simplifies to our normal formula for mean values. It is only when weights vary across individuals that weights must be explicitly addressed).\n",
    "\n",
    "In this data, weights are stored in the variable `perwt`, which is the number of people for which each observation is a stand-in (the inverse of that observation's sampling probability). \n",
    "\n",
    "What is the *weighted* average salary for employed Black Americans? Please round your answer to two decimal places.\n",
    "\n",
    "What is the *weighted* average salary for employed White Americans?  Please round your answer to two decimal places.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "\n",
    "Now calculate the weighted average income gap between employed *non-Hispanic* White Americans (see discussion in Exercise 8) and employed Black Americans. \n",
    "\n",
    "First, calculate the weighted average income for employed non-Hispanic White Americans. Please round your answer to two decimals.\n",
    "\n",
    "How much more does the average non-Hispanic White American make than the average Black American (in percentage terms)? (That is, what is the non-Hispanic White average salary minus the Black average salary divided by the Black average salary). You should get a number between 0 and 100. Please round it to one decimal place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Citation\n",
    "\n",
    "The ACS data used in this exercise are a subsample of the IPUMS USA data available from [usa.ipums.org.](usa.ipums.org)\n",
    "\n",
    "Please cite use of the data as follows: Steven Ruggles, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas, Megan Schouweiler and Matthew Sobek. IPUMS USA: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2021. https://doi.org/10.18128/D010.V11.0\n",
    "\n",
    "These data are intended for this exercise only. Individuals analyzing the data for other purposes must submit a separate data extract request directly via IPUMS USA.\n",
    "\n",
    "Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "718fed28bf9f8c7851519acf2fb923cd655120b36de3b67253eeb0428bd33d2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
