{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are not a Duke MIDS student:** The following exercises are, I'm afraid, specially tailored to Duke students. You *can* do them if you're not a Duke student, but you'll have to pay ðŸ’° to Azure. Sorry! My best guess is that this will cost <10 USD per person, though, so it shouldn't be too expensive to play with if you want to try it out with your own credit card!\n",
    "\n",
    "The plan for today is relatively simple: we're gonna take what we did in our [last exercise](cloud_azure_arcos.ipynb), and scale it up!\n",
    "\n",
    "To make this work, I've added one member from each pair from last week to an Azure Resource Group tied to a Duke MIDS Azure subscription. In other words: we're gonna actually pay for some compute today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** To begin, go into Azure and create a new Machine Learning Workspace under the `practicalds2020rg` Resource Group (which should now be an option in the Resource Group dropdown menu.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Now start a new Jupyter notebook and create a new Workspace object. Note that you will probably get an error saying you don't have access -- that's because you need to re-authenticate your computer so that Python is connecting to your Duke resources, not the resources you had access to under your free student account / free new user account. You can review details of how to do [this here](https://www.practicaldatascience.org/html/cloud_dask.html#Starting-a-Dask-Cluster).\n",
    "\n",
    "Note you will need a Subscription ID, which I'll provide in class. (if you're not at Duke, this will be your paid account Subscription ID). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Start up a new cluster just like in the last exercise, but this time instead of specifying `vm_size=\"Standard_DS11_v2` (small VMs with only 2 CPUs per node), set `vm_size=\"Standard_DS4_v2\"` (VMs with 8 CPUs per node), and set `initial_node_count=2` (don't worry, we'll scale that later).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Today we'll be working with the full 80GB 2008-2012 Washington Post Arcos dataset. To save time, I've already uploaded this data and made it public, so you can access it with the following specifications:\n",
    "\n",
    "```python \n",
    "storage_options = {'account_name': 'practicalds2020sa'}\n",
    "df = dd.read_csv('az://practicalds2020arcos/arcos_*.tsv',\n",
    "                 storage_options=storage_options)\n",
    "```\n",
    "\n",
    "**Note that I've broken the CSV into lots of smaller files!** When doing parallel work, it's *much* faster to break up the input file so that its easier for dask to assign different files to different workers instead of trying to load all the data through one file.\n",
    "\n",
    "So take the code you used to answer Question 9 in the last exercises (calculating opioid shipment quantities per county per year) and update it to work here. \n",
    "\n",
    "(If you're not at Duke, you can download [this data here](https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/?arc404=true) and upload yourself! Remember to unzip before upload.)\n",
    "\n",
    "**AS ALWAYS** start with a subset of the data (e.g. use `dd.read_csv(...).head(1000)`). Note that the one column you had before you won't find here is `year`, so you'll have to fix that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Now run that code on the full dataset and time it. Note you can time it using your watch / system clock, don't worry about trying to use a Python timer. Was that faster than when you did it by hand at home?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now the extra fun part! When you created a cluster, you should have gotten back a little widget that looked like this:\n",
    "\n",
    "![azure_cluster_widget.png](../images/azure_cluster_widget.png)\n",
    "\n",
    "(if you didn't, in a new cell just put the name of your cluster (e.g. `amlcluster`) and run the cell. If you get text not a widget, [see directions near the top of this reading](../cloud_dask.ipynb). \n",
    "\n",
    "Open the \"manual scaling\" tab in that widget, and set it to 6. WOO!\n",
    "\n",
    "Now if you check your dask dashboard, you should slowly see more workers appear (click the workers tab to see progress). Note this will take 5-10 minutes -- new computers are being setup for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** Once you have those workers, run your code again. How long did it take this time? Given we increased our the number of nodes we had from 2 to 6 (a 3x improvement), as the improvement more or less than you might have expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(8)** **CLOSE YOUR CLUSTER!** We're not on free accounts any more, so while your dask cluster will shut down on its own in a couple hours, please save Duke a little cash and shut it down explicitly with `amlcluster.close()` (if you called your cluster `amlcluster`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
