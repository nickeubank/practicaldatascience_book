{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: gathering, inspecting, and cleaning the data\n",
    "\n",
    "In this step of the project, we'll gather the data that we need.\n",
    "\n",
    "1. Carbon dioxide emissions by country\n",
    "2. Income (as measured by GDP per capita) by country\n",
    "3. Population by country (so we can convert CO2 emissions into per capita emissions)\n",
    "4. List of territories by continent (since we want to be able to group the countries by region of the world)\n",
    "\n",
    "As we load each dataset we will explore it and perform some pre-processing steps to prepare the data for merging it together for our plots. We'll be on the lookout for any inconsistencies in the data (numbers and text mixed up in the data), missing data which may dictate which years we will be able to plot in our final plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "Let's start by loading our four data sources. The first three sources are provided on the GapMinder website, but each has a slightly different source.\n",
    "\n",
    "1. [**Carbon dioxide emissions**](https://www.gapminder.org/tools/#$model$markers$spreadsheet$encoding$number$data$concept=co2_emissions_tonnes_per_person&space@=country&=time;;&scale$domain:null&type:null&zoomed:null;;&label$data$concept=name;;&frame$value=2018&data$concept=time;;;;;;&chart-type=spreadsheet&url=v1) (tonnes per person). These data are corbon dioxide emissions from the burning of fossil fuels in the units of metric tonnes pf CO2 per person. These data are sourced from the [Carbon Dioxide Information Analysis Center](https://cdiac.ess-dive.lbl.gov/) at Lawrence Berkeley National Laboratory. If you download these data as a CSV you will get the file 'co2_emissions_tonnes_per_person.csv', which we have included in the `data/raw/` folder as `co2_emissions_tonnes_per_person.csv`.\n",
    "2. [**Income per person**](https://www.gapminder.org/tools/#$model$markers$spreadsheet$encoding$number$data$concept=income_per_person_gdppercapita_ppp_inflation_adjusted&space@=country&=time;;&scale$domain:null&type:null&zoomed:null;;&label$data$concept=name;;&frame$value=2018&data$concept=time;;;;;;&chart-type=spreadsheet&url=v1) (gross domestic product per capita, in international dollars, inflation-adjusted to 2011 prices). These data are sourced from the Gapminder based on World Bank, A. Maddison, M. Lindgren, International Monetary Fund, and others:  [link to more information on the data source](https://www.gapminder.org/data/documentation/gd001/). If you download these data as a CSV you will get the file 'income_per_person_gdppercapita_ppp_inflation_adjusted.csv', which we have included in the `data/raw/` folder as `income_per_person_gdppercapita_ppp_inflation_adjusted.csv`.\n",
    "3. [**Population**](https://www.gapminder.org/tools/#$model$markers$spreadsheet$encoding$number$data$concept=population_total&space@=country&=time;;&scale$domain:null&type:null&zoomed:null;;&label$data$concept=name;;&frame$value=2018&data$concept=time;;;;;;&chart-type=spreadsheet&url=v1). These data are sourced from Gapminder based on Maddison and the United Nations: [link to more information on the data source](http://gapm.io/dpop). If you download these data as a CSV you will get the file 'population_total.csv', which we have included in the `data/raw/` folder as `population_total.csv`.\n",
    "4. [**Territories by continent**](https://unstats.un.org/unsd/methodology/m49/overview#). These data are sourced from the United Nations. These data are provided in the file 'united_nations_continents.csv' which we have included in the `data/raw/` folder as `united_nations_continents.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each of the above links you *could* have clicked the \"CSV\" button to grab the data, but we have done that for you and provided all of the data in the `data/` folder so you can get started exploring these data right away, but we have not changed the content of each dataset at all - that's up to you to use your programming skills to analyze these data! \n",
    "\n",
    "## The Goal\n",
    "\n",
    "Your immediate goal is to prepare the data so that they can be merged. This means you'll need to establish a common column of data by which you can merge the data. You'll also need to clean up all of the remaining columns so that the data are ready for use. We'll want to compare these data for the latest year that's common across all of the data. That's your big picture goal!\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "Before you begin modifying any dataset, you should get to know it a bit to understand its content, check for missing or anomalous values, and determine whether any additional processing is needed before you analyze it further. \n",
    "\n",
    "**(1)** Load each of the four datasets above. Does every dataset load or are some problematic? If you encounter issues, take a look at the raw file to see if you can figure out why and develop a solution that will allow you to load the data.\n",
    "\n",
    "**(2)** Once each of your datasets is loaded, view first 10-20 entries of each dataset. Note the headings and the values in the data. \n",
    "\n",
    "**(3)** What are the common years of data provided, and what's the latest year? That's the year we'll want to compare across and can ignore the other years for this project. Let's select that year from each of our datasets.\n",
    "\n",
    "**(4)** Are there any unexpected values of data? Pay particular attention to the income and population datasets. One thing to notice about the dataset is that the values in the data are not entirely numerical. There are values like '20k'. This is a symbol for a thousand (1,000 or k). There may be others in the dataset. We can't analyze the data like this. You'll need to first, find all the unique strings used in the data and then write a function that translates the strings into numerical values. If you'd like some guidance, here are two steps to think about in terms of how to do this, since it will need to be applied to a large number of cases:\n",
    "\n",
    "- Create a that determines what unique string values are present in a dataset to make sure we know everything that needs to be adjusted. Create a function that takes in a `numpy` array of values like '20k' and searches through them to find any alphabetic letters that are present. We recommend looking into the string method `isalpha()` and the `numpy` method `unique()` to help with this exercise. For example, if your array of strings contained the following:\n",
    "|string list|\n",
    "|-|\n",
    "| '13k' |\n",
    "| '546' |\n",
    "| '9M' |\n",
    "| '12M' |\n",
    "| '900k' |\n",
    "\n",
    "The desired output would be: ['k', 'M']\n",
    "\n",
    "- Write another function that takes a string as input and outputs the correct numerical representation of the number as a float. That is, if the input is '20' then the output should be the number 20.0; if the input is '20k' then the output should be the number 20000.0**\n",
    "\n",
    "Once you have the two cases above, apply these to your data to prepare them for use.\n",
    "\n",
    "**(5)** Once you have your data processed save them each to one of four files. This will be your intermediate product after the first round of processing. It's good practice to leave your raw data as it is and enable yourself to reproduce your data processing pipeline with code in case you need to make changes later. You can place them in a csv file using the pd.to_csv() method and read it back later. We'd recommend using the `index=False` keyword argument for `to_csv()` to prevent the creation of an extraneous column.\n",
    "\n",
    "**(6)** Create a function for processing each input into an intermediate product so that you can easily rerun your code.\n",
    "\n",
    "**BEFORE PROCEEDING IN THE COURSE, BE SURE TO WORK THROUGH THIS YOURSELF. YOU CAN DO THIS!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
